{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facebook Data Crawling\n",
    "In this notebook, we will be crawling data from Facebook using the Facebook Graph API. We will be using the facebook-scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the required library\n",
    "We will be using the facebook-scraper library to crawl data from Facebook. We will install this library using pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: facebook_scraper in c:\\users\\adb\\anaconda3\\lib\\site-packages (0.2.59)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: pandas in c:\\users\\adb\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\adb\\anaconda3\\lib\\site-packages (1.24.3)\n",
      "Requirement already satisfied: dateparser<2.0.0,>=1.0.0 in c:\\users\\adb\\anaconda3\\lib\\site-packages (from facebook_scraper) (1.1.8)\n",
      "Requirement already satisfied: demjson3<4.0.0,>=3.0.5 in c:\\users\\adb\\anaconda3\\lib\\site-packages (from facebook_scraper) (3.0.6)\n",
      "Requirement already satisfied: requests-html<0.11.0,>=0.10.0 in c:\\users\\adb\\anaconda3\\lib\\site-packages (from facebook_scraper) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\adb\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\adb\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in c:\\users\\adb\\anaconda3\\lib\\site-packages (from dateparser<2.0.0,>=1.0.0->facebook_scraper) (2022.7.9)\n",
      "Requirement already satisfied: tzlocal in c:\\users\\adb\\anaconda3\\lib\\site-packages (from dateparser<2.0.0,>=1.0.0->facebook_scraper) (5.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\adb\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: requests in c:\\users\\adb\\anaconda3\\lib\\site-packages (from requests-html<0.11.0,>=0.10.0->facebook_scraper) (2.29.0)\n",
      "Requirement already satisfied: pyquery in c:\\users\\adb\\anaconda3\\lib\\site-packages (from requests-html<0.11.0,>=0.10.0->facebook_scraper) (2.0.0)\n",
      "Requirement already satisfied: fake-useragent in c:\\users\\adb\\anaconda3\\lib\\site-packages (from requests-html<0.11.0,>=0.10.0->facebook_scraper) (1.3.0)\n",
      "Requirement already satisfied: parse in c:\\users\\adb\\anaconda3\\lib\\site-packages (from requests-html<0.11.0,>=0.10.0->facebook_scraper) (1.19.1)\n",
      "Requirement already satisfied: bs4 in c:\\users\\adb\\anaconda3\\lib\\site-packages (from requests-html<0.11.0,>=0.10.0->facebook_scraper) (0.0.1)\n",
      "Requirement already satisfied: w3lib in c:\\users\\adb\\anaconda3\\lib\\site-packages (from requests-html<0.11.0,>=0.10.0->facebook_scraper) (1.21.0)\n",
      "Requirement already satisfied: pyppeteer>=0.0.14 in c:\\users\\adb\\anaconda3\\lib\\site-packages (from requests-html<0.11.0,>=0.10.0->facebook_scraper) (1.0.2)\n",
      "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in c:\\users\\adb\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->facebook_scraper) (1.4.4)\n",
      "Requirement already satisfied: certifi>=2021 in c:\\users\\adb\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->facebook_scraper) (2023.5.7)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in c:\\users\\adb\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->facebook_scraper) (6.0.0)\n",
      "Requirement already satisfied: pyee<9.0.0,>=8.1.0 in c:\\users\\adb\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->facebook_scraper) (8.2.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in c:\\users\\adb\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->facebook_scraper) (4.65.0)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in c:\\users\\adb\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->facebook_scraper) (1.26.16)\n",
      "Requirement already satisfied: websockets<11.0,>=10.0 in c:\\users\\adb\\anaconda3\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->facebook_scraper) (10.4)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\adb\\anaconda3\\lib\\site-packages (from bs4->requests-html<0.11.0,>=0.10.0->facebook_scraper) (4.12.2)\n",
      "Requirement already satisfied: lxml>=2.1 in c:\\users\\adb\\anaconda3\\lib\\site-packages (from pyquery->requests-html<0.11.0,>=0.10.0->facebook_scraper) (4.9.2)\n",
      "Requirement already satisfied: cssselect>=1.2.0 in c:\\users\\adb\\anaconda3\\lib\\site-packages (from pyquery->requests-html<0.11.0,>=0.10.0->facebook_scraper) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\adb\\anaconda3\\lib\\site-packages (from requests->requests-html<0.11.0,>=0.10.0->facebook_scraper) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\adb\\anaconda3\\lib\\site-packages (from requests->requests-html<0.11.0,>=0.10.0->facebook_scraper) (3.4)\n",
      "Requirement already satisfied: tzdata in c:\\users\\adb\\anaconda3\\lib\\site-packages (from tzlocal->dateparser<2.0.0,>=1.0.0->facebook_scraper) (2023.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\adb\\anaconda3\\lib\\site-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->facebook_scraper) (3.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\adb\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.42.1->pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->facebook_scraper) (0.4.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\adb\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4->requests-html<0.11.0,>=0.10.0->facebook_scraper) (2.4)\n"
     ]
    }
   ],
   "source": [
    "%pip install facebook_scraper pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl the data using facebook_scraper\n",
    "Now we can get the data from Facebook using the facebook_scraper library. We will be using the get_posts function to get the posts from the fanpage. This function will return a list of dictionaries, where each dictionary represents a post. We will be saving this list of dictionaries to a json file. More information about what you can do with the facebook_scraper library can be found here: https://github.com/kevinzg/facebook-scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define variables\n",
    "First we have to define some variables that we will be using throughout the notebook. \n",
    "- FANPAGE_LINK: The link to the fanpage that we want to crawl data from. This can be found by going to the fanpage and copying the link from the address bar. For example, the link to the fanpage of the [Nintendo Switch](https://www.facebook.com/NintendoSwitch/) is https://www.facebook.com/NintendoSwitch/. We will be using this link as the value for FANPAGE_LINK.\n",
    "\n",
    "- COOKIE_PATH: The path to the cookie file that we will be using to authenticate with Facebook. This cookie file can be obtained by logging into Facebook and copying the cookie from the browser. For example, in Chromium, use extension [Get cookies.txt LOCALLY](https://chrome.google.com/webstore/detail/get-cookiestxt/bgaddhkoddajcdgocldbbfleckgcbcid) to get the cookie file. Then save the cookie to a file and use the path to this file as the value for COOKIE_PATH. <span style=\"color:red; font-weight:bold\">USE COOKIE FROM A FAKE ACCOUNT, OTHERWISE YOUR REAL ACCOUNT MIGHT GET BANNED.</span>.\n",
    "\n",
    "\n",
    "- FOLDER_NAME: The name of the folder that we will be saving the data to. This folder will be created in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from facebook_scraper import get_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from facebook_scraper import *\n",
    "\n",
    "results = []\n",
    "start_url = None\n",
    "def handle_pagination_url(url):\n",
    "    global start_url\n",
    "    start_url = url\n",
    "set_cookies(\"www.facebook.com_cookies.txt\")\n",
    "while True:\n",
    "    try:\n",
    "        for post in get_posts(\"tuyenvanhoaOfficial\", page_limit=None, start_url=start_url, request_url_callback=handle_pagination_url,\n",
    "                    options={\"comments\": True, \"reactions\": True, \"allow_extra_requests\": True},\n",
    "                    extra_info=True):\n",
    "            print(len(results))\n",
    "            results.append(post)\n",
    "        print(\"All done\")\n",
    "        break\n",
    "    except exceptions.TemporarilyBanned:\n",
    "        print(\"Temporarily banned, sleeping for 10m\")\n",
    "        time.sleep(600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert list of dicts to df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can convert the list of dictionaries to a pandas dataframe. We will be using the pandas library to do this. We will also be saving the dataframe to a xlxs or csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./TuyenVanHoaOfficial.csv\n"
     ]
    }
   ],
   "source": [
    "# Initialize dataframe to scrape Facebook post\n",
    "post_df_full = pd.DataFrame(columns=post_list[0].keys(), index=range(len(post_list)), data=post_list)\n",
    "\n",
    "# To df\n",
    "path=FOLDER_PATH + FANPAGE_LINK + \".csv\"\n",
    "post_df_full.to_csv(path, index=False)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df_full"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
